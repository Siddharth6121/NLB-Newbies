{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e6944e-7d77-4744-9886-19a4cbaef158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 17:19:38.590617: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 17:19:40.648919: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2024-03-23 17:19:40.833452: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2245900000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 12s 52ms/step - loss: 0.7591 - accuracy: 0.8448 - val_loss: 0.3396 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89525, saving model to model_without_duplicates.h5\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 8s 49ms/step - loss: 0.2694 - accuracy: 0.9128 - val_loss: 0.2776 - val_accuracy: 0.9086\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.89525 to 0.90861, saving model to model_without_duplicates.h5\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 8s 49ms/step - loss: 0.1676 - accuracy: 0.9413 - val_loss: 0.2732 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.90861 to 0.91008, saving model to model_without_duplicates.h5\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 9s 49ms/step - loss: 0.1465 - accuracy: 0.9473 - val_loss: 0.2945 - val_accuracy: 0.9103\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.91008 to 0.91028, saving model to model_without_duplicates.h5\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 9s 50ms/step - loss: 0.1287 - accuracy: 0.9528 - val_loss: 0.2781 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.91028 to 0.91589, saving model to model_without_duplicates.h5\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 8s 49ms/step - loss: 0.1213 - accuracy: 0.9549 - val_loss: 0.2898 - val_accuracy: 0.9145\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.91589\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 8s 49ms/step - loss: 0.1122 - accuracy: 0.9580 - val_loss: 0.2898 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.91589\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 9s 49ms/step - loss: 0.1109 - accuracy: 0.9581 - val_loss: 0.3011 - val_accuracy: 0.9132\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.91589\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 8s 49ms/step - loss: 0.1028 - accuracy: 0.9617 - val_loss: 0.3272 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.91589\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 9s 49ms/step - loss: 0.1019 - accuracy: 0.9620 - val_loss: 0.3460 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.91589\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 33) for input KerasTensor(type_spec=TensorSpec(shape=(None, 33), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 31).\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.5480 - accuracy: 0.8543\n",
      "Test Loss: 0.5480355620384216\n",
      "Test Accuracy: 0.854314386844635\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 33) for input KerasTensor(type_spec=TensorSpec(shape=(None, 33), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 31).\n",
      "[[2 0 1 1 3 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 3 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 1 3 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 2 0 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 0 1 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[2 2 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 0 1 0 0 0 0 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 2 0 0 1 0 0 0 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 2 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[2 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 3 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 2 0 0 0 0 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 2 0 0 1 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 0 0 1 1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 3 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"train_without_duplicates.csv\")\n",
    "# df.append(pd.read_csv(\"validate_without_duplicates.csv\"))\n",
    "\n",
    "# df = pd.read_csv(\"train_without_duplicates.csv\")\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Review'])\n",
    "\n",
    "# Define a ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(\"model_without_duplicates.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels = {'PRED': 1, 'PROD1': 2, 'ASP': 3, 'PROD2': 4}\n",
    "count = 0\n",
    "labels_numbered = []\n",
    "for _, row in df.iterrows():\n",
    "    m_len = len(sequences[count])+2\n",
    "    output_sequence = [0] * m_len\n",
    "    for label, indices in eval(row['Label']).items():\n",
    "\n",
    "        # Set the values according to the class\n",
    "        for index in indices:\n",
    "            output_sequence[index] = labels[label]\n",
    "            \n",
    "    labels_numbered.append(output_sequence)\n",
    "    count+=1\n",
    "\n",
    "# Pad sequences and handle null entries\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "labels_padded = pad_sequences(labels_numbered, maxlen=max_length, padding='post')\n",
    "\n",
    "# Step 2: Define the LSTM Model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Step 3: Training the Model\n",
    "X = sequences_padded\n",
    "y = labels_padded\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "# Step 4: Evaluation (if needed)\n",
    "# Evaluate the model on a separate validation set or use cross-validation techniques\n",
    "# Load and preprocess the validation dataset\n",
    "validation_df = pd.read_csv(\"test_without_duplicates.csv\")  # Replace \"validation_data.csv\" with your validation dataset file name\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_df['Review'])\n",
    "max_length = max(len(seq) for seq in validation_sequences)\n",
    "validation_sequences_padded = pad_sequences(validation_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "validation_labels = {'PRED': 1, 'PROD1': 2, 'ASP': 3, 'PROD2': 4}\n",
    "count = 0\n",
    "validation_labels_numbered = []\n",
    "for _, row in validation_df.iterrows():\n",
    "    # m_len = len(validation_sequences[count])+5\n",
    "    m_len = len(validation_sequences_padded[count])\n",
    "    output_sequence = [0] * m_len\n",
    "    for label, indices in eval(row['Label']).items():\n",
    "        # Set the values according to the class\n",
    "        for index in indices:\n",
    "            output_sequence[index] = labels[label]\n",
    "            \n",
    "    validation_labels_numbered.append(output_sequence)\n",
    "    count+=1\n",
    "\n",
    "\n",
    "validation_labels_padded = pad_sequences(validation_labels_numbered, maxlen=max_length, padding='post')\n",
    "\n",
    "X_val = validation_sequences_padded\n",
    "y_val = validation_labels_padded\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "##################################################################################################################\n",
    "# Step 5: Prediction\n",
    "# Load the saved model\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"model_without_duplicates.h5\")\n",
    "\n",
    "\n",
    "# Example sentence to predict SRL labels\n",
    "example_sentences = [\"it is much less expensive than a router also\",\n",
    "                    \"good price  same performance as sony\",\"i purchased this to replace a less expensive headset i got from dell\",\n",
    "                    \"this is an excellent camera from sony\",\n",
    "                     \"the picture cd  while lower total resolution  was far sharper than anything the 8400f could produce at any size\",\n",
    "\"i have used the canon cartridges for many years and thought i would try to save money buy purchasing this less expensive brand which has been very disappointing\",\n",
    "\"this camera  though a bit less known than those other big-brand versions is and was one of the best qualtiy cameras i've used\",\n",
    "\"canon has better sound with video recording  less noise  better detail in shadows\",\n",
    "\"this detector is basically the same as the escort passport 8500  but less expensive\",\n",
    "\"this is a great camera and has many unique features usually found on more expensive or larger digital cameras\",\n",
    "\"yet this camera is less expensive than the newest model\",\n",
    "\"the battery life is good and i always believe in genuine replacements from the manufacturer even if they are slightly more expensive\",\n",
    "\"oh  it has two settings  the 1500watt and the 900 watt and the fan is the same speed on both\",\n",
    "\"usps is so much faster than ups\",\n",
    "\"the card works great and the quality is great and the price is even greater\",\n",
    "\"my computer is working so much faster now\",\n",
    "\"they still read in it's directory as  shn files this device is extremely versatile and worth far more than the low price amazon's asking\",\n",
    "\"sure  you could spend more and get an ipod  but why would you want to when this device is so much less expensive\",\n",
    "\"this model's oe radio is not as deep as the newer aftermarket radios\",\n",
    "\"no other camera has the same range of features for the same price\",\n",
    "\"the radio reception is great  and much better than cheap digital tuner walkmans   weighs next to nothing and comes with a convenient plastic proective case\",\n",
    "\"the camera creates a larger file through interpolation  but the 6mp image is actually of worse resolution than 3 mp image\",\n",
    "\"i understand what it takes to make a good stereo  and i know what value there is in the more expensive kits\"]\n",
    "\n",
    "for example_sentence in example_sentences:\n",
    "    # Tokenize the example sentence\n",
    "    example_sequence = tokenizer.texts_to_sequences([example_sentence])\n",
    "\n",
    "    # Pad the sequence to ensure it has the same length as the sequences used during training\n",
    "    example_sequence_padded = pad_sequences(example_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_labels = np.argmax(loaded_model.predict(example_sequence_padded),axis=-1)\n",
    "    # predicted_labels = loaded_model.predict(example_sequence_padded)\n",
    "    print(predicted_labels)\n",
    "    # print(predicted_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3e8f2-448f-4362-9e5a-53790a509541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410fa9f-793b-4d31-8605-fc74cc39d112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_NEWBIES",
   "language": "python",
   "name": "nlpnewbies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
